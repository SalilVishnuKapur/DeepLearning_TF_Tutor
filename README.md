# MachineLearning-DeepLearning_Tutor
This is a project for Machine Learning practitioners who want to advance their ML skills to the extent where they can easily gain Hands On Experience. It is divided into 3 main modules which basically define the  :-

##################
# Neural Networks#
##################

The focus for this module is to teach how to implement the following Deep learning models using tensorflow:-

0. LinearRegression(LR)
Linear Regression is a regression technique used to find the exact values.




1. Artificial Neural Network(ANN)
Artificial Neural network has been quite useful for the problems where the data has some non linearity. So this Neural Network model can find out patterns from data having some non linearity and proceed.




2. Convolutional Neural Network(CNN)
Concept of Convolution was given by Yan Nikun as, if we had to process all the features of the image it would take a lot of time and processing capabilities. Convolution step reduces the  number of features by picking only the important ones and proceeding. But now this state of art architecture has been replaced by the Capsule Network architecture proposed by Geofrey Hinton in Sept, 2017.




3. Recurrent Neural Network(RNN)
Now this Neural Network saves the memory as it goes on, but in this architecture there is no possibility of removing the saved information. So there is no freedom of customizing the saving process of the memory. 




4. Long Short Term Memory(LSTM)
These Neural Networks are for sequential data or time series data. These type of neural networks have a memory for saving the information as they process the data and used this saved memory information to get insights from the data. Like by training a LSTM with the paragraph data one can make a Neural Network model to get the predictions about the next word if we pass this model a single line to process.
Ref :- https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html 


5. Capsule Neural Networks(CNN)
These are the Neural Networks which check the features of images so that there order is also correct, rather than just looking at the images for the features. For the example we can think about a situtaion where the image with the eyebrows, nose or lips are displaced, in this case capsule network will let you know that the image is something else and not a male or female.

###########
#Ensembles# 
###########

The focus of this module is to teach how to implement these algorithms from scratch following the ensemble approach :-

0. Decision Tree
A decision tree is a decision support tool that uses a tree-like graph or model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm that only contains conditional control statements.


1. Random Forest
Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.

#######################
#Validation Algorithms#
#######################

These algorithms are to validate how good is the model :-

0. Cross Validation 
Cross-validation, sometimes called rotation estimation,[1][2][3] is a model validation technique for assessing how the results of a statisticalanalysis will generalize to an independent data set. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice.


1. Stratified Cross Validation
Stratification is the process of rearranging the data as to ensure each fold is a good representative of the whole. For example in a binary classification problem where each class comprises 50% of the data, it is best to arrange the data such that in every fold, each class comprises around half the instances.
